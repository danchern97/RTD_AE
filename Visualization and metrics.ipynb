{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gudhi as gd\n",
    "import gudhi.wasserstein as wasserstein\n",
    "import gudhi.hera as hera\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import combinations, combinations_with_replacement, product\n",
    "\n",
    "import ripserplusplus as rpp\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'viridis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f577cc6",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b1680",
   "metadata": {},
   "source": [
    "### Many models - many datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0082062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [\n",
    "#     'Synthetic/Circle', \n",
    "#     'Synthetic/2Clusters', \n",
    "#     'Synthetic/3Clusters', \n",
    "# # #     'Synthetic/Infty', \n",
    "#     'Synthetic/RandomCube'\n",
    "# ]\n",
    "\n",
    "datasets = [\n",
    "    'MNIST',\n",
    "    'F-MNIST', \n",
    "    'COIL-20',\n",
    "    'scRNA_mice',\n",
    "    'scRNA_melanoma'\n",
    "#     'c.elegans'\n",
    "]\n",
    "\n",
    "models = {\n",
    "    'umap':'UMAP',\n",
    "    'tsne':'t-SNE',\n",
    "    'pacmap':'PaCMAP',\n",
    "    'phate':'PHATE',\n",
    "    'ivis':'Ivis',\n",
    "    'Basic AutoEncoder':'AE',\n",
    "    'Topological AutoEncoder':'TopoAE (Moor et.al.)',\n",
    "    'RTD AutoEncoder H1':'RTD'\n",
    "}\n",
    "dataset_names = {'RandomCube':'Random', '2Clusters':'2 Clusters', '3Clusters':'3 Clusters'}\n",
    "#'scRNA melanoma':'scRNA melanoma', 'scRNA_mice':'scRNA mice', \n",
    "versions = {\n",
    "    'scRNA_melanoma':'d2', \n",
    "    'scRNA_mice':'d2',\n",
    "    'COIL-20':'d2', \n",
    "    'F-MNIST':'d2', \n",
    "    'MNIST':'d2'}\n",
    "add_original = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(latent, labels, alpha=0.7, title=\"\", fontsize=25, s=2.0, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    scatter = ax.scatter(latent[:, 0], latent[:, 1], alpha=alpha, c=labels, s=s, label=labels)\n",
    "#     legend = ax.legend(*scatter.legend_elements(num=len(np.unique(labels))), loc=\"upper left\", title=\"Types\")\n",
    "#     ax.add_artist(legend)\n",
    "    if len(title):\n",
    "        ax.set_title(title, fontsize=fontsize)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975f9dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot multiple datasets latent representations\n",
    "fig, axes = plt.subplots(len(datasets), len(models)+int(add_original), figsize=((len(models)+int(add_original))*6, len(datasets)*6), squeeze=False)\n",
    "for i, dataset in enumerate(datasets):\n",
    "    version = versions.get(dataset, \"\")\n",
    "    print(f\"dataset: {dataset}, version: {version}\")\n",
    "    labels = None # refactor\n",
    "    try:\n",
    "        labels = np.load(f\"data/{dataset}/prepared/train_labels.npy\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        labels = np.load(f\"data/{dataset}/prepared/labels.npy\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    if add_original:\n",
    "        original_data = np.load(f\"data/{dataset}/prepared/train_data.npy\")\n",
    "        axes[i][0].scatter(original_data[:, 0], original_data[:, 1], c=labels, s=20.0, alpha=0.7, cmap=plt.cm.get_cmap('nipy_spectral', 11))\n",
    "        if i == 0:\n",
    "            axes[0][0].set_title('Original data', fontsize=40)\n",
    "        axes[i][0].tick_params(\n",
    "            axis='both', \n",
    "            which='both', \n",
    "            bottom=False, \n",
    "            top=False,\n",
    "            labelbottom=False,\n",
    "            right=False,\n",
    "            left=False,\n",
    "            labelleft=False\n",
    "        )\n",
    "        d = dataset.split('/')[-1]\n",
    "        d = dataset_names.get(d, d)\n",
    "        axes[i][0].set_ylabel(d, fontsize=40)\n",
    "    for j, name in enumerate(models):\n",
    "        if add_original:\n",
    "            j+=1\n",
    "            \n",
    "        latent = None\n",
    "        \n",
    "        potential_filenames = [\n",
    "            f'data/{dataset}/{name}_output_{version}.npy', \n",
    "            f'data/{dataset}/{name}_output_d2.npy', \n",
    "            f'data/{dataset}/{name}_output_.npy',\n",
    "            f'data/{dataset}/{name}_output.npy'\n",
    "        ]\n",
    "        for n in potential_filenames:\n",
    "            try:\n",
    "                latent = np.load(n)\n",
    "                break\n",
    "            except FileNotFoundError:\n",
    "                print(\"\")\n",
    "        if latent is None:\n",
    "            raise FileNotFoundError(f'No file for model: {name}, dataset: {dataset}')\n",
    "        axes[i][j].scatter(latent[:, 0], latent[:, 1], c=labels, s=20.0, alpha=0.7, cmap=plt.cm.get_cmap('nipy_spectral', 11))\n",
    "        if i == 0:\n",
    "            axes[i][j].set_title(f'{models[name]}', fontsize=40)\n",
    "        if j == 0 and not add_original:\n",
    "            d = dataset.split('/')[-1]\n",
    "            d = dataset_names.get(d, d)\n",
    "            axes[i][j].set_ylabel(d, fontsize=40)\n",
    "        axes[i][j].tick_params(\n",
    "            axis='both', \n",
    "            which='both', \n",
    "            bottom=False, \n",
    "            top=False,\n",
    "            labelbottom=False,\n",
    "            right=False,\n",
    "            left=False,\n",
    "            labelleft=False\n",
    "        )\n",
    "plt.savefig('results/real.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9da3e6",
   "metadata": {},
   "source": [
    "### Many datasets - original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "datasets = [\n",
    "    'Synthetic/Circle', \n",
    "    'Synthetic/2Clusters', \n",
    "    'Synthetic/3Clusters', \n",
    "    'Synthetic/RandomCube'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(datasets), figsize=(len(datasets)*6, 6*1))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    labels = None # refactor\n",
    "    try:\n",
    "        labels = np.load(f\"data/{dataset}/prepared/train_labels.npy\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        labels = np.load(f\"data/{dataset}/prepared/labels.npy\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        data = np.load(f'data/{dataset}/prepared/train_data.npy')\n",
    "    except FileNotFoundError:\n",
    "        data = np.load(f'data/{dataset}/prepared/data.npy')\n",
    "    name = dataset.split('/')[-1]\n",
    "    axes[i].scatter(data[:, 0], data[:, 1], c=labels, s=60.0, alpha=0.7, cmap=plt.cm.get_cmap('nipy_spectral', 11))\n",
    "    axes[i].set_title(name, fontsize=30)\n",
    "    axes[i].tick_params(\n",
    "        axis='both', \n",
    "        which='both', \n",
    "        bottom=False, \n",
    "        top=False,\n",
    "        labelbottom=False,\n",
    "        right=False,\n",
    "        left=False,\n",
    "        labelleft=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9dbd00",
   "metadata": {},
   "source": [
    "### One dataset - many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'COIL-20'\n",
    "\n",
    "models = [\n",
    "#     'umap':'UMAP',\n",
    "#     'tsne':'t-SNE',\n",
    "#     'Basic AutoEncoder':'AE',\n",
    "#     'Topological AutoEncoder':'TopoAE (Moor et.al.)',\n",
    "#     ('umap', '', 'UMAP'),\n",
    "#     ('tsne', '', 't-SNE'),\n",
    "    ('RTD AutoEncoder H1', 'geodesic', 'RTD-AE g'),\n",
    "    ('RTD AutoEncoder H1 min_max', '3d', 'RTD-AE MM'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d05b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(models), figsize=(len(models)*6, 6))\n",
    "for j, m in enumerate(models):\n",
    "    name, version, print_name = m\n",
    "#     version = versions.get(name, \"\")\n",
    "    latent = np.load(f'data/{dataset_name}/{name}_output_{version}.npy')\n",
    "    try:\n",
    "        labels = np.load(f'data/{dataset_name}/{name}_labels_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        labels = np.ones(latent.shape[0])\n",
    "    if latent.shape[1] > 2:\n",
    "        print(f\"Error: {name}\")\n",
    "    axes[j].scatter(latent[:, 0], latent[:, 1], c=labels, s=20.0, alpha=0.7, cmap=plt.cm.get_cmap('nipy_spectral', 11))\n",
    "    axes[j].set_title(f'{print_name}', fontsize=30)\n",
    "    axes[j].tick_params(\n",
    "        axis='both', \n",
    "        which='both', \n",
    "        bottom=False, \n",
    "        top=False,\n",
    "        labelbottom=False,\n",
    "        right=False,\n",
    "        left=False,\n",
    "        labelleft=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82d3dc",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'COIL-20'\n",
    "version = 'd16'\n",
    "\n",
    "models = {\n",
    "#     'pca':'PCA',\n",
    "#     'mds':'MDS',\n",
    "    'phate':'PHATE',\n",
    "#     'pacmap':'PaCMAP',\n",
    "#     'ivis':'Ivis',\n",
    "#     'umap':'UMAP',\n",
    "#     'tsne':'t-SNE',\n",
    "#     'Basic AutoEncoder':'AE',\n",
    "#     'Topological AutoEncoder':'TopoAE (Moor et.al.)',\n",
    "#     'RTD AutoEncoder H1':'RTD-AE'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306043c0",
   "metadata": {},
   "source": [
    "## Calculate distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pdist_gpu(a, b, device = 'cuda:1'):\n",
    "    A = torch.tensor(a, dtype = torch.float64)\n",
    "    B = torch.tensor(b, dtype = torch.float64)\n",
    "\n",
    "    size = (A.shape[0] + B.shape[0]) * A.shape[1] / 1e9\n",
    "    max_size = 0.2\n",
    "\n",
    "    if size > max_size:\n",
    "        parts = int(size / max_size) + 1\n",
    "    else:\n",
    "        parts = 1\n",
    "\n",
    "    pdist = np.zeros((A.shape[0], B.shape[0]))\n",
    "    At = A.to(device)\n",
    "\n",
    "    for p in range(parts):\n",
    "        i1 = int(p * B.shape[0] / parts)\n",
    "        i2 = int((p + 1) * B.shape[0] / parts)\n",
    "        i2 = min(i2, B.shape[0])\n",
    "\n",
    "        Bt = B[i1:i2].to(device)\n",
    "        pt = torch.cdist(At, Bt)\n",
    "        pdist[:, i1:i2] = pt.cpu()\n",
    "\n",
    "        del Bt, pt\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del At\n",
    "\n",
    "    return pdist\n",
    "\n",
    "def zero_out_diagonal(distances):# make 0 on diagonal\n",
    "    return distances * (np.ones_like(distances) - np.eye(*distances.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "data = data.reshape(data.shape[0], -1)\n",
    "ids = np.random.choice(np.arange(len(data)), size=min(30000, len(data)), replace=False)\n",
    "data = data[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distances = pdist_gpu(data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distances = zero_out_diagonal(original_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20754cfe",
   "metadata": {},
   "source": [
    "## Pearson correlation for pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/train_labels.npy')\n",
    "except FileNotFoundError:\n",
    "    labels = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "# ids = np.random.choice(np.arange(0, len(labels)), size=min(6000, len(labels)), replace=False)\n",
    "\n",
    "def get_distances(data):\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    distances = distance_matrix(data, data)\n",
    "    distances = distances[np.triu(np.ones_like(distances), k=1) > 0]\n",
    "    return distances\n",
    " # take only different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model_name in models:\n",
    "    try:\n",
    "        latent = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')[ids]\n",
    "        latent_distances = pdist_gpu(latent, latent)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    results[model_name] = pearsonr(\n",
    "        latent_distances[np.triu(np.ones_like(original_distances), k=1) > 0], \n",
    "        original_distances[np.triu(np.ones_like(original_distances), k=1) > 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')\n",
    "x = latent[:, 0]\n",
    "y = latent[:, 1]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b1784",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(f\"data/{dataset_name}/prepared/train_labels.npy\")\n",
    "is_there_test = True\n",
    "try:\n",
    "    test_labels = np.load(f\"data/{dataset_name}/prepared/test_labels.npy\")\n",
    "except FileNotFoundError:\n",
    "    is_there_test = False\n",
    "    train_ids, test_ids = train_test_split(np.arange(0, len(train_labels)), test_size=0.2)\n",
    "    test_labels = train_labels[test_ids]\n",
    "    train_labels = train_labels[train_ids]\n",
    "    \n",
    "results = defaultdict(dict)\n",
    "\n",
    "for model_name in models:\n",
    "    try:\n",
    "        train_data = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    if is_there_test:\n",
    "        test_data = np.load(f'data/{dataset_name}/{model_name}_output_{version}_test.npy')\n",
    "    else:\n",
    "        test_data = train_data[test_ids]\n",
    "        train_data = train_data[train_ids]\n",
    "#         print('test_data not found')\n",
    "#     for k in [3, 5, 10, 50, 100]:\n",
    "#         classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "#         classifier.fit(train_data, train_labels)\n",
    "#         results[model_name][f'knn_k{k}'] = accuracy_score(test_labels, classifier.predict(test_data))\n",
    "    C = 1.0\n",
    "    classifier = SVC(C=C, kernel='rbf')\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    results[model_name][f'svm_C{C}'] = accuracy_score(test_labels, classifier.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f66ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489374d",
   "metadata": {},
   "source": [
    "## Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_persistence_gd(distances, dim=3):\n",
    "    skeleton = gd.RipsComplex(distance_matrix = distances)\n",
    "    simplex_tree = skeleton.create_simplex_tree(max_dimension=dim)\n",
    "    barcodes = simplex_tree.persistence()\n",
    "    pbarcodes = {}\n",
    "    for i in range(dim+1):\n",
    "        pbarcodes[i] = [[b[1][0], b[1][1]] for b in barcodes if b[0] == i]\n",
    "    return pbarcodes\n",
    "\n",
    "\n",
    "def cast_to_normal_array(barcodes):\n",
    "    return np.array([[b, d] for b, d in barcodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 5\n",
    "batch_size = 2048\n",
    "\n",
    "data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "if batch_size > len(data):\n",
    "    n_runs=1\n",
    "    \n",
    "max_dim = 1\n",
    "results = defaultdict(dict)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    ids = np.random.choice(np.arange(0, len(original_distances)), size=min(batch_size, len(original_distances)), replace=False)\n",
    "    \n",
    "    x = data[ids]\n",
    "#     distances = distance_matrix(x, x)\n",
    "    distances = original_distances[ids][:, ids]/np.percentile(original_distances.flatten(), 90)\n",
    "    barcodes = {'original':rpp.run(f'--format distance --dim {max_dim}', data=distances)}\n",
    "    datasets = {'original':x}\n",
    "    \n",
    "    for model_name in tqdm(models):\n",
    "        try:\n",
    "            x = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')\n",
    "            x = x.reshape(len(x), -1)[ids]\n",
    "            distances = zero_out_diagonal(pdist_gpu(x, x))\n",
    "            distances = distances/np.percentile(distances.flatten(), 90)\n",
    "            if not np.isnan(distances).any():\n",
    "                barcodes[model_name] = rpp.run(f'--format distance --dim {max_dim}', data=distances)\n",
    "            datasets[model_name] = x\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        for dim in range(max_dim+1):\n",
    "            if model_name not in results[dim]:\n",
    "                results[dim][model_name] = []\n",
    "            results[dim][model_name].append(hera.wasserstein_distance(\n",
    "                cast_to_normal_array(barcodes['original'][dim]), \n",
    "                cast_to_normal_array(barcodes[model_name][dim]),\n",
    "                internal_p=1.0\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50fe59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dim in range(max_dim+1):\n",
    "    print(f'Dimension: {dim}')\n",
    "    for model_name in results[dim]:\n",
    "        print(f\"Model: {model_name}: {np.mean(results[dim][model_name]):.3f} $\\pm$ {np.std(results[dim][model_name]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115d4e9",
   "metadata": {},
   "source": [
    "## Triplet accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ccdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'COIL-20'\n",
    "# version = \"geodesic\"\n",
    "# models = {\n",
    "#     'umap':'UMAP',\n",
    "#     'tsne':'t-SNE',\n",
    "#     'Basic AutoEncoder':'AE',\n",
    "#     'Topological AutoEncoder':'TopoAE (Moor et.al.)',\n",
    "#     'RTD AutoEncoder H1':'RTD-AE-H1',\n",
    "#     'RTD AutoEncoder H2':'RTD-AE-H2'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_accuracy(input_data, latent_data, triplets=None):\n",
    "    # calculate distance matricies\n",
    "    input_data = input_data.reshape(input_data.shape[0], -1)\n",
    "    input_distances = zero_out_diagonal(pdist_gpu(input_data, input_data))\n",
    "    latent_data = latent_data.reshape(latent_data.shape[0], -1)\n",
    "    latent_distances = zero_out_diagonal(pdist_gpu(latent_data, latent_data))\n",
    "    # generate triplets\n",
    "    if triplets is None:\n",
    "        triplets = np.asarray(list(combinations(range(len(input_data)), r=3)))\n",
    "    i_s = triplets[:, 0]\n",
    "    j_s = triplets[:, 1]\n",
    "    k_s = triplets[:, 2]\n",
    "    acc = (np.logical_xor(\n",
    "        input_distances[i_s, j_s] < input_distances[i_s, k_s], \n",
    "        latent_distances[i_s, j_s] < latent_distances[i_s, k_s]\n",
    "    ) == False)\n",
    "    acc = np.mean(acc.astype(np.int32))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def avg_triplet_accuracy(input_data, latent_data, batch_size=128, n_runs=20):\n",
    "    # average over batches\n",
    "    accs = []\n",
    "    triplets = np.asarray(list(combinations(range(min(batch_size, len(input_data))), r=3)))\n",
    "    if batch_size > len(input_data):\n",
    "        accs.append(triplet_accuracy(input_data, latent_data, triplets=triplets))\n",
    "        return accs\n",
    "    for _ in range(n_runs):\n",
    "        ids = np.random.choice(np.arange(len(input_data)), size=batch_size, replace=False)\n",
    "        accs.append(triplet_accuracy(input_data[ids], latent_data[ids], triplets=triplets))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6716f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "\n",
    "for model_name in models:\n",
    "    try:\n",
    "        latent_data = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    accs = avg_triplet_accuracy(input_data, latent_data, batch_size=150, n_runs=10)\n",
    "    print(f\"Model: {model_name}, triplet acc: {np.mean(accs):.3f} $\\pm$ {np.std(accs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d68d1",
   "metadata": {},
   "source": [
    "# RTD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dab6f",
   "metadata": {},
   "source": [
    "Switch to ripser++ from ArGentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rtd import RTDLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e3b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "batch_size = 200\n",
    "\n",
    "loss = RTDLoss(dim=1, engine='ripser')\n",
    "\n",
    "data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "data = data.reshape(len(data), -1)\n",
    "\n",
    "if batch_size > len(data):\n",
    "    n_runs=1\n",
    "    \n",
    "max_dim = 1\n",
    "results = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "    ids = np.random.choice(np.arange(0, len(data)), size=min(batch_size, len(data)), replace=False)\n",
    "    \n",
    "    x = data[ids]\n",
    "    x_distances = distance_matrix(x, x)\n",
    "    x_distances = x_distances/np.percentile(x_distances.flatten(), 90)\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            z = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                z = np.load(f'data/{dataset_name}/{model_name}_output.npy')\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        z = z.reshape(len(z), -1)[ids]\n",
    "        z_distances = distance_matrix(z, z)\n",
    "        z_distances = z_distances/np.percentile(z_distances.flatten(), 90)\n",
    "        print(f'Calculating RTD for: {model_name}')\n",
    "        with torch.no_grad():\n",
    "            _, _, value = loss(torch.tensor(x_distances), torch.tensor(z_distances))\n",
    "        results[model_name].append(value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'pca', \n",
    "    'mds', \n",
    "    'tsne', \n",
    "    'umap', \n",
    "    'Basic AutoEncoder', \n",
    "    'pacmap', \n",
    "    'ivis', \n",
    "    'phate', \n",
    "    'Topological AutoEncoder', \n",
    "    'RTD AutoEncoder H1'\n",
    "]\n",
    "for model_name in names:\n",
    "    if model_name in results:\n",
    "        print(f\"{model_name}: {np.mean(results[model_name]):.2f} $\\pm$ {np.std(results[model_name]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a032104",
   "metadata": {},
   "source": [
    "# Tripet acc. between cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_distances(data, labels):\n",
    "    clusters = []\n",
    "    if len(data.shape) > 2:\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "    for l in np.sort(np.unique(labels)):\n",
    "        clusters.append(np.mean(data[labels == l], axis=0))\n",
    "    clusters = np.asarray(clusters)\n",
    "    return distance_matrix(clusters, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(f'data/{dataset_name}/prepared/train_data.npy')\n",
    "labels = np.load(f'data/{dataset_name}/prepared/train_labels.npy')\n",
    "original_distances = get_cluster_distances(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc59f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_accuracy_between_clusters(original_distances, latent_distances):\n",
    "    triplets = np.asarray(list(combinations(range(len(original_distances)), r=3)))\n",
    "    i_s = triplets[:, 0]\n",
    "    j_s = triplets[:, 1]\n",
    "    k_s = triplets[:, 2]\n",
    "    acc = (np.logical_xor(\n",
    "        original_distances[i_s, j_s] < original_distances[i_s, k_s], \n",
    "        latent_distances[i_s, j_s] < latent_distances[i_s, k_s]\n",
    "    ) == False)\n",
    "    return acc\n",
    "\n",
    "def triplet_accuracy_between_clusters_(original_distances, latent_distances):\n",
    "    ids = range(len(original_distances))\n",
    "    triplets = np.asarray(list(product(ids, ids, ids)))\n",
    "    i_s = triplets[:, 0]\n",
    "    j_s = triplets[:, 1]\n",
    "    k_s = triplets[:, 2]\n",
    "    acc = (np.logical_xor(\n",
    "        original_distances[i_s, j_s] < original_distances[i_s, k_s], \n",
    "        latent_distances[i_s, j_s] < latent_distances[i_s, k_s]\n",
    "    ) == False)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    try:\n",
    "        latent_data = np.load(f'data/{dataset_name}/{model_name}_output_{version}.npy')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    latent_distances = get_cluster_distances(latent_data, labels)\n",
    "    accs = triplet_accuracy_between_clusters_(original_distances, latent_distances)\n",
    "    print(f\"Model: {model_name}, triplet acc: {np.mean(accs):.3f} $\\pm$ {np.std(accs):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
